# NLPErrorCorrection

This repository is an extension of the paper Combating Adversarial Misspellings with Robust Word Recognition authored by Danish Pruthi, Bhuwan Dhingra, Zachary C. Lipton. The paper details a character level BiLSTM to correct spelling mistakes. It uses a background model trained on a larger corpus of general data to back up a more focused model on a certain topic. The foreground model could be trained only on text message data. This model would have a small vocabulary, but have expertise in the topic it is looking at. The background model would be trained on a wider corpus. It would have less domain knowledge but it would have a longer tail vocabulary and may be able to save the foreground model when an unknown word is presented to it. The datasets used for this are the Stanford Sentiment Tree bank. These are ~12,000 movie reviews that have been annotated into trees and give a sentiment score 1 - 25. We discard the tree and convert the score to binary for our downstream tasks. The second is IMDB movie reviews. This is a larger corpus of text ~50,000 reviews. 

We replicated the paper as well as extended it. We updated the architecture to us an encoder only transformer (BERT) as our network. This by itself increased the accuracy of error correction. We also implement a novel loss function based on edit distance, Levenshtien distance. This loss is comuptationally expensive, so we only train the foreground model with it. The edit distance punishes the model severly for suggesting words that are far away from the presented letters and requires it to focus on the nearby words. As we limit the amount of mistakes to one or two we do not penalize the model for suggesting words that have a Levenshtien distance smaller than or equal to two. The word error rate under this scheme of a transformer foreground model trained with Levenshtien regularization and a transformer background model was 5.0, 1.9 percent better than the previous paper.
